{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of Retrieval:\n",
    "1. Naive Retrieval \n",
    "2. Sentench Window Retrieval \n",
    "3. Self Query Retrieval\n",
    "4. Parent Document Retrieval\n",
    "5. HDE (Hypothetical Document Embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid Search : Naive Retrieval + Keyword Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Autonomous vehicles rely on precise detection of road signs and signals.\",\n",
    "    \"Artificial Intelligence plays a critical role in the development of autonomous driving technologies.\",\n",
    "    \"Advanced algorithms, such as those used in autonomous driving systems, require versatile programming languages like Python.\",\n",
    "    \"Traffic sign detection is a fundamental component of autonomous driving systems.\",\n",
    "    \"Data science is essential for analyzing sensor data and improving autonomous driving accuracy.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"autonomous driving systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_query =  preprocess_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vector.fit_transform(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.26926948,\n",
       "       0.        , 0.        , 0.15903489, 0.        , 0.        ,\n",
       "       0.        , 0.26926948, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.22351799, 0.33375258,\n",
       "       0.        , 0.33375258, 0.        , 0.        , 0.33375258,\n",
       "       0.        , 0.33375258, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.33375258, 0.33375258, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.33375258, 0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = vector.transform([preprocessed_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(X, query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.069312  ],\n",
       "       [0.16035627],\n",
       "       [0.29382252],\n",
       "       [0.41485198],\n",
       "       [0.13893868]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_indices = np.argsort(similarities, axis=0)[::-1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 4, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_documents = [documents[i] for i in ranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autonomous driving systems'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked1: Traffic sign detection is a fundamental component of autonomous driving systems.\n",
      "Ranked2: Advanced algorithms, such as those used in autonomous driving systems, require versatile programming languages like Python.\n",
      "Ranked3: Artificial Intelligence plays a critical role in the development of autonomous driving technologies.\n",
      "Ranked4: Data science is essential for analyzing sensor data and improving autonomous driving accuracy.\n",
      "Ranked5: Autonomous vehicles rely on precise detection of road signs and signals.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(ranked_documents):\n",
    "    print(f\"Ranked{i+1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = np.array([\n",
    "    [0.112, 0.422, 0.312, 0.491, 0.645],\n",
    "    [0.553, 0.126, 0.625, 0.701, 0.725],\n",
    "    [0.821, 0.859, 0.901, 0.953, 0.991]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = np.array([[0.1, 0.2, 0.3, 0.4, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(document_embeddings,query_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_indices = np.argsort(similarities, axis= 0)[::-1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = 'D:\\Algo_lab\\langchain_essentials\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size =200, chunk_overlap = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\Algo_lab\\\\langchain_essentials\\\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf', 'page': 0}, page_content='DetectionandRecognitionofBangladeshiTrafficSigninReal-worldImages\\nSyedSaminSadaf\\n200042163\\nMizbaulHaqueMaruf\\n200042125\\nSyedTamzidBakth\\n200042145\\nDepartmentofComputerScienceandEngineering'),\n",
       " Document(metadata={'source': 'D:\\\\Algo_lab\\\\langchain_essentials\\\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf', 'page': 0}, page_content='IslamicUniversityofTechnology\\nAugust,2024'),\n",
       " Document(metadata={'source': 'D:\\\\Algo_lab\\\\langchain_essentials\\\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf', 'page': 1}, page_content='Contents\\n1 Introduction 1\\n1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1'),\n",
       " Document(metadata={'source': 'D:\\\\Algo_lab\\\\langchain_essentials\\\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf', 'page': 1}, page_content='1.3 ProblemStatement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\\n1.4 ResearchChallenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2'),\n",
       " Document(metadata={'source': 'D:\\\\Algo_lab\\\\langchain_essentials\\\\Traffic_Sign_Detection_on_Dhaka_Traffic_Sign_Dataset.pdf', 'page': 1}, page_content='1.5 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.6 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n2 RelatedWorks 4')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInferenceAPIEmbeddings(api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(chunks):\n",
    "    if not doc.page_content:\n",
    "        print(f\"Empty content in document {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = set(chunks[0].metadata.keys())\n",
    "for i, doc in enumerate(chunks):\n",
    "    if set(doc.metadata.keys()) != keys:\n",
    "        print(f\"Inconsistent metadata in document {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.embeddings.huggingface.HuggingFaceInferenceAPIEmbeddings'>\n"
     ]
    }
   ],
   "source": [
    "print(type(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(chunks)))  # Ensure IDs match the number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2604\\1236997188.py\", line 4, in <module>\n",
      "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
      "  File \"d:\\Algo_lab\\langchain_essentials\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 878, in from_documents\n",
      "    return cls.from_texts(\n",
      "  File \"d:\\Algo_lab\\langchain_essentials\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 842, in from_texts\n",
      "    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "  File \"d:\\Algo_lab\\langchain_essentials\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 295, in add_texts\n",
      "    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n",
      "  File \"d:\\Algo_lab\\langchain_essentials\\venv\\lib\\site-packages\\langchain_community\\vectorstores\\chroma.py\", line 295, in <listcomp>\n",
      "    [embeddings[idx] for idx in non_empty_ids] if embeddings else None\n",
      "KeyError: 0\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "try:\n",
    "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
    "except KeyError as e:\n",
    "    print(\"KeyError:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
